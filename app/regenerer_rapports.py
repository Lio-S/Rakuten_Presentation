import pandas as pd
import numpy as np
from preprocess import ProductClassificationPipeline, PipelineConfig
from utils import safe_read_csv
import os
import logging

def regenerer_rapport_modele(model_type, pipeline, nouveaux_libelles):
    """
    Reg√©n√®re un rapport pour un mod√®le donn√©
    
    Args:
        model_type (str): Type de mod√®le ('neural_net', 'xgboost', 'SVM')
        pipeline: Instance du pipeline avec les donn√©es charg√©es
        nouveaux_libelles (dict): Dictionnaire des nouveaux libell√©s
    
    Returns:
        pd.DataFrame: Rapport g√©n√©r√©
    """
    print(f"üîÑ R√©g√©n√©ration du rapport {model_type}...")
    
    try:
        # 1) Charger le mod√®le sp√©cifique
        print(f"üß† Chargement du mod√®le {model_type}...")
        if model_type == 'SVM':
            # Pour SVM, utiliser le mod√®le texte  
            pipeline.load_text_model('SVM')
            pipeline.prepare_text_data()
            print("‚úÖ Mod√®le SVM (texte) charg√©")
        else:
            # Pour neural_net et xgboost, utiliser les mod√®les image
            pipeline.load_model(model_type)
            print(f"‚úÖ Mod√®le {model_type} charg√©")
        
        # ‚ö†Ô∏è FORCER les nouveaux libell√©s apr√®s chargement (√©crase les anciens sauvegard√©s dans le mod√®le)
        anciens_libelles = pipeline.category_names.copy() if hasattr(pipeline, 'category_names') else {}
        pipeline.category_names = nouveaux_libelles
        print(f"üîÑ Libell√©s forc√©s vers les nouveaux pour {model_type}")
        
        # Afficher quelques comparaisons anciennes vs nouvelles
        if anciens_libelles and model_type != 'SVM':  # Pour les mod√®les image seulement
            print("   üìã Comparaison libell√©s (quelques exemples):")
            for code in list(nouveaux_libelles.keys())[:3]:
                ancien = anciens_libelles.get(code, 'Non d√©fini')
                nouveau = nouveaux_libelles.get(code, 'Non d√©fini')
                if ancien != nouveau:
                    print(f"      Code {code}: '{ancien}' ‚Üí '{nouveau}'")
        
        # 2) Pr√©parer les donn√©es selon le type de mod√®le
        if model_type == 'SVM':
            # Donn√©es texte
            X_test = pipeline.text_test_data
            y_test = pipeline.text_test_labels
            test_indices = pipeline.preprocessed_data['test_split_indices']
            
            # Pr√©dictions texte
            predictions, probabilities = pipeline.predict_text(X_test)
            
        else:
            # Donn√©es image
            X_test_split = pipeline.preprocessed_data['X_test_split']
            y_test_split = pipeline.preprocessed_data['y_test_split']
            test_indices = pipeline.preprocessed_data['test_split_indices']
            
            # Gestion du format des donn√©es
            if isinstance(X_test_split, dict):
                X_test_split = X_test_split['features']
            elif X_test_split.shape == ():
                X_test_split = X_test_split.item()['features']
                
            # Pr√©dictions image
            predictions, probabilities = pipeline.predict(X_test_split)
            y_test = y_test_split
        
        print(f"‚úÖ Pr√©dictions {model_type} g√©n√©r√©es pour {len(predictions)} √©chantillons")
        
        # 3) Cr√©er le DataFrame du rapport
        print("üìä Cr√©ation du rapport...")
        
        # Charger les donn√©es originales pour r√©cup√©rer imageid
        X_train_df = safe_read_csv('../data/X_train_update.csv')
        
        rapport_data = []
        for i, test_idx in enumerate(test_indices):
            if test_idx in X_train_df.index:
                image_info = X_train_df.loc[test_idx]
                
                # R√©cup√©ration des donn√©es
                predicted_category = predictions[i]
                true_category = y_test[i]
                prediction_probability = np.max(probabilities[i])
                
                # G√©n√©ration des noms de cat√©gories avec NOUVEAUX libell√©s
                predicted_category_name = nouveaux_libelles.get(predicted_category, f"Cat√©gorie {predicted_category}")
                true_category_name = nouveaux_libelles.get(true_category, f"Cat√©gorie {true_category}")
                
                # V√©rification que les nouveaux libell√©s sont utilis√©s
                if i == 0:  # Premi√®re it√©ration seulement
                    print(f"   üîç V√©rification libell√©s - Code {predicted_category} ‚Üí '{predicted_category_name}'")
                
                rapport_data.append({
                    'original_index': i,
                    'imageid': image_info['imageid'],
                    'predicted_category': predicted_category,
                    'prediction_probability': round(prediction_probability, 8),
                    'true_category': true_category,
                    'predicted_category_name': predicted_category_name,
                    'true_category_name': true_category_name
                })
        
        # Cr√©er le DataFrame
        rapport_df = pd.DataFrame(rapport_data)
        
        # 4) Sauvegarder le nouveau fichier
        if model_type == 'SVM':
            output_file = '../data/reports/rapport_text_SVM_nouveau.csv'
        else:
            output_file = f'../data/reports/rapport_{model_type.lower()}_nouveau.csv'
            
        rapport_df.to_csv(output_file, index=False)
        
        print(f"‚úÖ Rapport sauvegard√© dans {output_file}")
        print(f"üìà Statistiques du rapport {model_type}:")
        print(f"   - Nombre d'√©chantillons: {len(rapport_df)}")
        print(f"   - Codes de cat√©gories utilis√©s: {sorted(rapport_df['predicted_category'].unique())}")
        print(f"   - Pr√©cision: {(rapport_df['predicted_category'] == rapport_df['true_category']).mean():.4f}")
        
        # 5) V√©rifier que tous les codes ont des libell√©s
        codes_sans_libelle = []
        for code in rapport_df['predicted_category'].unique():
            if nouveaux_libelles.get(code, '').startswith('Cat√©gorie ') or nouveaux_libelles.get(code, '') == '':
                codes_sans_libelle.append(code)
                
        if codes_sans_libelle:
            print(f"\n‚ö†Ô∏è ATTENTION: Codes sans libell√© d√©fini: {codes_sans_libelle}")
        else:
            print(f"\n‚úÖ Tous les codes {model_type} ont des libell√©s d√©finis")
            
        return rapport_df
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©g√©n√©ration {model_type}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def regenerer_tous_les_rapports():
    """
    Reg√©n√®re tous les rapports (SVM, neural_net, xgboost)
    """
    print("üöÄ R√©g√©n√©ration de tous les rapports...")
    
    try:
        # 1) Configuration et initialisation du pipeline
        config = PipelineConfig(
            data_path='../data',
            model_path='../data/models', 
            image_dir='../data/images',
            batch_size=128,
            target_size=2000,
            num_workers=7
        )
        
        pipeline = ProductClassificationPipeline(config)
        print("‚úÖ Pipeline initialis√©")
        
        # 2) Charger les donn√©es pr√©process√©es
        print("üìÅ Chargement des donn√©es pr√©process√©es...")
        pipeline.preprocessed_data = pipeline.prepare_data_streamlit()
        print("‚úÖ Donn√©es charg√©es")
        
        # 3) FORCER les nouveaux libell√©s (avant de charger les mod√®les)
        nouveaux_libelles = {
            1: "Livres occasion",
            4: "Jeux consoles neuf", 
            5: "Accessoires gaming",
            6: "Consoles de jeux",
            13: "Mod√©lisme",
            114: "Objets pop culture",
            116: "Cartes de jeux",
            118: "Jeux de r√¥le et figurines",
            128: "Jouets enfant",
            132: "Pu√©riculture",
            156: "Mobilier",
            192: "Linge de maison",
            194: "√âpicerie",
            206: "D√©coration",
            222: "Animalerie",
            228: "Journaux et revues occasion",
            1281: "Jeux enfant", 
            1301: "Lingerie enfant et jeu de bar",
            1302: "Jeux et accessoires de plein air",
            2403: "Lots livres et magazines",
            2462: "Console et Jeux vid√©os occasion",
            2522: "Fournitures papeterie",
            2582: "Mobilier et accessoires de jardin",
            2583: "Piscine et accessoires",
            2585: "Outillage de jardin",
            2705: "Livres neufs",
            2905: "Jeux PC en t√©l√©chargement"
        }
        
        # 3) Liste des mod√®les √† traiter
        modeles = ['SVM', 'neural_net', 'xgboost']
        rapports = {}
        
        # 4) Reg√©n√©rer chaque rapport
        for model_type in modeles:
            print(f"\n{'='*50}")
            print(f"üìä TRAITEMENT DU MOD√àLE {model_type.upper()}")
            print(f"{'='*50}")
            
            try:
                rapport = regenerer_rapport_modele(model_type, pipeline, nouveaux_libelles)
                rapports[model_type] = rapport
                
                # Afficher quelques exemples
                print(f"\nüìã Exemples du rapport {model_type}:")
                print(rapport.head(5).to_string(index=False))
                
            except Exception as e:
                print(f"‚ùå Erreur avec le mod√®le {model_type}: {str(e)}")
                rapports[model_type] = None
        
        # 5) Sauvegarder les indices utilis√©s (une seule fois)
        sauvegarder_indices_utilises(pipeline)
        
        # 6) R√©sum√© final
        print(f"\n{'='*50}")
        print("üìä R√âSUM√â FINAL")
        print(f"{'='*50}")
        
        for model_type, rapport in rapports.items():
            if rapport is not None:
                precision = (rapport['predicted_category'] == rapport['true_category']).mean()
                print(f"‚úÖ {model_type}: {len(rapport)} √©chantillons, pr√©cision: {precision:.4f}")
            else:
                print(f"‚ùå {model_type}: √âchec de la g√©n√©ration")
        
        print(f"\nüìÅ FICHIERS G√âN√âR√âS:")
        print("üìã Rapports principaux:")
        for model_type in modeles:
            prefix = "text_" if model_type == 'SVM' else ""
            print(f"   ‚úÖ ../data/reports/rapport_{prefix}{model_type.lower()}_nouveau.csv")
            
        print("\nüìä Analyses d'erreurs:")
        for model_type in modeles:
            print(f"   ‚úÖ ../data/erreurs/analyse_erreurs_{model_type.lower()}.json")
            print(f"   ‚úÖ ../data/erreurs/erreurs_par_classe_{model_type.lower()}.csv")
            print(f"   ‚úÖ ../data/erreurs/matrice_confusion_{model_type.lower()}.csv")
            
        print("\nüíæ Pr√©dictions:")
        for model_type in modeles:
            prefix = "text_" if model_type == 'SVM' else ""
            print(f"   ‚úÖ ../data/predictions/predictions_{prefix}{model_type.lower()}.csv")
            
        print("\nüîç Explications:")
        for model_type in modeles:
            print(f"   ‚úÖ ../data/explanations/explications_{model_type.lower()}.json")
            
        print("\nüìã Indices:")
        print("   ‚úÖ ../data/indices/indices_utilises.json")
        print("   ‚úÖ ../data/indices/test_indices.csv")
        
        return rapports
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©g√©n√©ration compl√®te: {str(e)}")
        import traceback
        traceback.print_exc()
        raise

def analyser_erreurs_predictions(model_type, pipeline, nouveaux_libelles, predictions, probabilities, y_true, test_indices):
    """
    G√©n√®re les analyses d'erreurs d√©taill√©es pour un mod√®le
    
    Args:
        model_type (str): Type de mod√®le 
        pipeline: Instance du pipeline
        nouveaux_libelles (dict): Nouveaux libell√©s des cat√©gories
        predictions: Pr√©dictions du mod√®le
        probabilities: Probabilit√©s des pr√©dictions
        y_true: Vraies √©tiquettes
        test_indices: Indices de test
        
    Returns:
        dict: Analyses d'erreurs g√©n√©r√©es
    """
    print(f"üìä Analyse des erreurs pour {model_type}...")
    
    try:
        from sklearn.metrics import confusion_matrix, classification_report
        import json
        
        # 1) Matrice de confusion
        cm = confusion_matrix(y_true, predictions)
        
        # 2) Rapport de classification d√©taill√©
        class_report = classification_report(y_true, predictions, output_dict=True, zero_division=0)
        
        # 3) Analyse par classe
        analyses_par_classe = {}
        for class_code in nouveaux_libelles.keys():
            if class_code in y_true or class_code in predictions:
                # Indices des √©chantillons de cette classe
                true_indices = np.where(y_true == class_code)[0]
                pred_indices = np.where(predictions == class_code)[0]
                
                if len(true_indices) > 0:
                    # Pr√©dictions correctes/incorrectes pour cette classe
                    correct_mask = (y_true[true_indices] == predictions[true_indices])
                    
                    # Confiances pour cette classe
                    class_probs = probabilities[true_indices]
                    if len(class_probs.shape) > 1:
                        # Trouver l'indice de la classe dans les probabilit√©s
                        if hasattr(pipeline, 'category_to_idx'):
                            class_prob_idx = pipeline.category_to_idx.get(class_code, 0)
                        else:
                            class_prob_idx = 0
                        confidence_scores = class_probs[:, class_prob_idx] if class_probs.shape[1] > class_prob_idx else class_probs[:, 0]
                    else:
                        confidence_scores = class_probs
                    
                    analyses_par_classe[class_code] = {
                        'nom_classe': nouveaux_libelles[class_code],
                        'nb_echantillons': len(true_indices),
                        'nb_correct': int(np.sum(correct_mask)),
                        'nb_incorrect': int(len(true_indices) - np.sum(correct_mask)),
                        'precision': float(correct_mask.mean()) if len(correct_mask) > 0 else 0.0,
                        'confiance_moyenne': float(np.mean(confidence_scores)) if len(confidence_scores) > 0 else 0.0,
                        'confiance_min': float(np.min(confidence_scores)) if len(confidence_scores) > 0 else 0.0,
                        'confiance_max': float(np.max(confidence_scores)) if len(confidence_scores) > 0 else 0.0,
                        'echantillons_faible_confiance': int(np.sum(confidence_scores < 0.5)) if len(confidence_scores) > 0 else 0
                    }
        
        # 4) Analyse globale des erreurs
        correct_predictions = (y_true == predictions)
        max_probs = np.max(probabilities, axis=1)
        
        analyse_globale = {
            'accuracy': float(np.mean(correct_predictions)),
            'nb_total_echantillons': len(y_true),
            'nb_correct': int(np.sum(correct_predictions)),
            'nb_incorrect': int(np.sum(~correct_predictions)),
            'confiance_moyenne_globale': float(np.mean(max_probs)),
            'confiance_moyenne_correct': float(np.mean(max_probs[correct_predictions])) if np.sum(correct_predictions) > 0 else 0.0,
            'confiance_moyenne_incorrect': float(np.mean(max_probs[~correct_predictions])) if np.sum(~correct_predictions) > 0 else 0.0,
            'nb_predictions_faible_confiance': int(np.sum(max_probs < 0.5))
        }
        
        # 5) Sauvegarder les analyses
        error_analysis = {
            'model_type': model_type,
            'analyse_globale': analyse_globale,
            'analyses_par_classe': analyses_par_classe,
            'matrice_confusion': cm.tolist(),
            'rapport_classification': class_report,
            'nouveaux_libelles': nouveaux_libelles
        }
        
        # Sauvegarder dans le dossier erreurs
        os.makedirs('../data/erreurs', exist_ok=True)
        
        # Fichier JSON principal
        with open(f'../data/erreurs/analyse_erreurs_{model_type.lower()}.json', 'w', encoding='utf-8') as f:
            json.dump(error_analysis, f, indent=2, ensure_ascii=False)
        
        # Fichier CSV d√©taill√© par classe
        classes_df = pd.DataFrame.from_dict(analyses_par_classe, orient='index')
        classes_df.index.name = 'code_classe'
        classes_df.to_csv(f'../data/erreurs/erreurs_par_classe_{model_type.lower()}.csv')
        
        # Matrice de confusion en CSV
        cm_df = pd.DataFrame(cm)
        cm_df.to_csv(f'../data/erreurs/matrice_confusion_{model_type.lower()}.csv')
        
        print(f"‚úÖ Analyses d'erreurs {model_type} sauvegard√©es")
        return error_analysis
        
    except Exception as e:
        print(f"‚ùå Erreur analyse erreurs {model_type}: {str(e)}")
        return {}

def sauvegarder_predictions_detaillees(model_type, predictions, probabilities, test_indices):
    """
    Sauvegarde les pr√©dictions dans le format attendu par Streamlit
    """
    print(f"üíæ Sauvegarde pr√©dictions {model_type}...")
    
    try:
        os.makedirs('../data/predictions', exist_ok=True)
        
        # DataFrame principal
        pred_data = {
            'prediction': predictions,
            'test_index': test_indices[:len(predictions)]  # S'assurer que les tailles correspondent
        }
        
        # Ajouter les probabilit√©s
        for i in range(probabilities.shape[1]):
            pred_data[f'prob_class_{i}'] = probabilities[:, i]
        
        pred_df = pd.DataFrame(pred_data)
        
        # Sauvegarder
        if model_type == 'SVM':
            filename = f'../data/predictions/predictions_text_{model_type.lower()}.csv'
        else:
            filename = f'../data/predictions/predictions_{model_type.lower()}.csv'
            
        pred_df.to_csv(filename, index=False)
        print(f"‚úÖ Pr√©dictions {model_type} sauvegard√©es dans {filename}")
        
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde pr√©dictions {model_type}: {str(e)}")

def generer_explications_modele(model_type, pipeline, nouveaux_libelles):
    """
    G√©n√®re les fichiers d'explication pour l'interpr√©tabilit√©
    """
    print(f"üîç G√©n√©ration explications {model_type}...")
    
    try:
        os.makedirs('../data/explanations', exist_ok=True)
        
        explications = {
            'model_type': model_type,
            'categories': nouveaux_libelles,
            'nb_classes': len(nouveaux_libelles),
            'architecture': 'Neural Network' if model_type == 'neural_net' else model_type.upper()
        }
        
        # Feature importance si disponible
        if hasattr(pipeline.model, 'feature_importances_'):
            explications['feature_importances'] = pipeline.model.feature_importances_.tolist()
            
        # Infos sur le mod√®le
        if hasattr(pipeline.model, 'get_params'):
            explications['parametres_modele'] = pipeline.model.get_params()
            
        # Sauvegarder
        import json
        with open(f'../data/explanations/explications_{model_type.lower()}.json', 'w', encoding='utf-8') as f:
            json.dump(explications, f, indent=2, ensure_ascii=False)
            
        print(f"‚úÖ Explications {model_type} g√©n√©r√©es")
        
    except Exception as e:
        print(f"‚ùå Erreur explications {model_type}: {str(e)}")

def sauvegarder_indices_utilises(pipeline):
    """
    Sauvegarde les indices utilis√©s pour la reproductibilit√©
    """
    print("üìã Sauvegarde des indices utilis√©s...")
    
    try:
        os.makedirs('../data/indices', exist_ok=True)
        
        indices_info = {
            'test_split_indices': pipeline.preprocessed_data['test_split_indices'].tolist(),
            'nb_test_samples': len(pipeline.preprocessed_data['test_split_indices']),
            'train_indices': pipeline.preprocessed_data['train_indices'].tolist() if 'train_indices' in pipeline.preprocessed_data else [],
            'nb_train_samples': len(pipeline.preprocessed_data['train_indices']) if 'train_indices' in pipeline.preprocessed_data else 0
        }
        
        # Sauvegarder en JSON et CSV
        import json
        with open('../data/indices/indices_utilises.json', 'w') as f:
            json.dump(indices_info, f, indent=2)
            
        # CSV pour les indices de test
        test_df = pd.DataFrame({'test_index': pipeline.preprocessed_data['test_split_indices']})
        test_df.to_csv('../data/indices/test_indices.csv', index=False)
        
        print("‚úÖ Indices sauvegard√©s")
        
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde indices: {str(e)}")
    """
    Compare les anciens et nouveaux rapports pour tous les mod√®les
    """
    try:
        print("\nüîç Comparaison avec les anciens rapports...")
        
        for model_type in modeles:
            print(f"\nüìä COMPARAISON {model_type.upper()}:")
            print("-" * 40)
            
            # Noms des fichiers
            if model_type == 'SVM':
                ancien_file = '../data/reports/rapport_text_SVM.csv'
                nouveau_file = '../data/reports/rapport_text_SVM_nouveau.csv'
            else:
                ancien_file = f'../data/reports/rapport_{model_type.lower()}.csv'
                nouveau_file = f'../data/reports/rapport_{model_type.lower()}_nouveau.csv'
            
            # Charger l'ancien rapport si il existe
            if os.path.exists(ancien_file):
                ancien_df = safe_read_csv(ancien_file)
                print(f"   üìÇ Ancien: {len(ancien_df)} √©chantillons")
                
                # Afficher les codes uniques dans l'ancien rapport
                if 'predicted_category' in ancien_df.columns:
                    anciens_codes = ancien_df['predicted_category'].unique()
                    print(f"   üìã Anciens codes: {sorted(anciens_codes)[:10]}...")
                
                if os.path.exists(nouveau_file):
                    nouveau_df = safe_read_csv(nouveau_file)
                    nouveaux_codes = nouveau_df['predicted_category'].unique()
                    print(f"   üìã Nouveaux codes: {sorted(nouveaux_codes)[:10]}...")
                    
                    # Comparaison des libell√©s
                    print("   üîÑ Exemples de changements:")
                    for i in range(min(3, len(ancien_df), len(nouveau_df))):
                        ancien_libelle = ancien_df.iloc[i].get('predicted_category_name', 'N/A')
                        nouveau_libelle = nouveau_df.iloc[i]['predicted_category_name']
                        if ancien_libelle != nouveau_libelle:
                            print(f"      Ligne {i}: '{ancien_libelle}' ‚Üí '{nouveau_libelle}'")
                else:
                    print(f"   ‚ö†Ô∏è Nouveau fichier {nouveau_file} non trouv√©")
            else:
                print(f"   üìÇ Ancien fichier {ancien_file} non trouv√©")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la comparaison: {str(e)}")

def nettoyer_et_remplacer_fichiers(confirmer=False):
    """
    Remplace les anciens fichiers par les nouveaux (optionnel)
    """
    if not confirmer:
        print("\n‚ùì Pour remplacer les anciens fichiers, ex√©cutez:")
        print("   nettoyer_et_remplacer_fichiers(confirmer=True)")
        return
        
    print("\nüîÑ Remplacement des anciens fichiers...")
    
    remplacements = [
        ('../data/reports/rapport_text_SVM_nouveau.csv', '../data/reports/rapport_text_SVM.csv'),
        ('../data/reports/rapport_neural_net_nouveau.csv', '../data/reports/rapport_neural_net.csv'),
        ('../data/reports/rapport_xgboost_nouveau.csv', '../data/reports/rapport_xgboost.csv')
    ]
    
    for nouveau, ancien in remplacements:
        if os.path.exists(nouveau):
            try:
                # Sauvegarder l'ancien comme backup
                if os.path.exists(ancien):
                    backup = ancien.replace('.csv', '_backup.csv')
                    os.rename(ancien, backup)
                    print(f"   üì¶ Backup cr√©√©: {backup}")
                
                # Remplacer par le nouveau
                os.rename(nouveau, ancien)
                print(f"   ‚úÖ {ancien} remplac√©")
                
            except Exception as e:
                print(f"   ‚ùå Erreur remplacement {ancien}: {e}")
        else:
            print(f"   ‚ö†Ô∏è {nouveau} non trouv√©")

if __name__ == "__main__":
    # Configuration logging pour r√©duire le bruit
    logging.getLogger('classification_pipeline').setLevel(logging.WARNING)
    
    # Reg√©n√©rer tous les rapports
    rapports = regenerer_tous_les_rapports()
    
    # Comparer avec les anciens
    comparer_anciens_nouveaux_rapports()
    
    print("\nüéØ R√©g√©n√©ration termin√©e!")
    print("   - Nouveaux fichiers cr√©√©s avec le suffixe '_nouveau'")
    print("   - V√©rifiez les r√©sultats avant de remplacer les anciens")
    print("   - Pour remplacer: nettoyer_et_remplacer_fichiers(confirmer=True)")
    
    # Afficher les fichiers cr√©√©s
    nouveaux_fichiers = [
        '../data/reports/rapport_text_SVM_nouveau.csv',
        '../data/reports/rapport_neural_net_nouveau.csv', 
        '../data/reports/rapport_xgboost_nouveau.csv'
    ]
    
    print(f"\nüìÅ Fichiers g√©n√©r√©s:")
    for fichier in nouveaux_fichiers:
        if os.path.exists(fichier):
            print(f"   ‚úÖ {fichier}")
        else:
            print(f"   ‚ùå {fichier} (non cr√©√©)")